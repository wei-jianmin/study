《虚拟化原理与实现》
    第一章  开篇
        通过时间、空间上的分时及模拟，虚拟化可以将一份资源抽象成多份。
        1.1 形形色色的虚拟化
            分层结构：
                计算机系统被分成了多个自下而上的层次，
                每一层都向上层呈现一个抽象，每一层只需知道下层抽象的接口，而无需了解其内部运作机制
                HAL（Hardware Abstraction Layer，硬件抽象层）
                    HAL是计算机中软件所能控制的硬件的抽象接口，
                    通常包括cpu的各种寄存器、内存管理模块、i/o端口、i/o内存映射的地址等
                API抽象层
                    抽象一个进程所能控制的系统功能的集合，包括如：
                    创建新进程、内存申请与释放、进程同步、文件操作、网络操作等
        1.2 系统虚拟化
            根据《现代操作系统》中的相关知识简单总结如下：
                操作系统跟硬件的交互，如寄存器读写、io读写、内存管理、中断管理等，
                都需要在特权模式下才能正常执行，
                但上面这些需要特权的指令，对于x86系的cpu来说，如果在保护模式下支持时，
                有些指令的确会引发中断，这是我们期望的，但还有一些指令会被cpu忽略
        1.3 系统虚拟化简史
            历史上第一个虚拟系统是IBM开发的
            但 x86体系的CPU，因为先天设计的问题，之前一直存在对系统虚拟化的支持缺陷或“虚拟化漏洞”
            所以，那时在x86体系结构上的虚拟化技术，都需要软件的方法来弥补系统结构设计上的不足。
            但这种靠软件实现虚拟化的方法，会带来性能上的损失以及非常大的软件复杂度，
            所以后来又提出一种类虚拟化技术，主要思想是
        1.4 系统虚拟化的好处
    第二章  x86架构及操作系统
        2.1 x86的历史和操作系统概要
            1978年       8086        16位寄存器+16位数据总线+20位地址寻址
            1982年       80286       引入保护模式
            1985年       80386       32位寄存器，引入虚拟内存
            1989年       80486       采用5级流水线，引入一级缓存
            1993年       奔腾        进一步增大一级缓存并将其分为指令缓存和数据缓存两部分，引入MMX多媒体技术
            1995~1999年  P6家族      采用了超标量技术
            2002~2006年  奔腾4       基于NetBurst微处理器架构，引入超线程，在奔腾4的662、672处理器上加入inter VT虚拟化支持
            2006年       Core系列
        2.2 x86内存架构
            地址空间
                物理地址
                    物理地址空间的大小，由CPU实现的物理地址位数所决定，物理地址位数和CPU处理数据的能力（即CPU位数）没有必然关系，
                    如16位的CPU具有20位的地址空间。内存和其他硬件设备分布在物理地址空间中。
                线性地址
                    每个进程都认为自己独享整个平台的硬件资源，为了让多个进程能够相互隔离的使用物理地址空间的资源，引入了线性空间的概念，
                    线性地址空间的大小，取决于CPU实现的线性地址位数。线性地址空间的大小与物理地址没有必然关系。
                    线性地址空间会被映射到物理地址空间的一部分（其他的在硬盘上）或整个物理地址空间。
                    cpu负责将线性地址空间转换为物理地址空间。
                    每个进程的线性地址空间，都是从0开始的，其内包括程序的代码段&数据段、用户堆、共享库、用户栈、系统内核等部分。
                    在未启用内存分页机制时，线性地址=物理地址。（如果未启用分页机制，则程序必须完全放入内存）
                逻辑地址（又称虚拟地址）
                    由于x86特殊的段机制，因此还有另一种地址空间：逻辑地址
                    该地址才是程序内部代码直接使用的地址。
                    逻辑地址由一个16位的段选择器和一个32位的偏移量（32位平台）构成。
                    段机制的由来
                        https://blog.csdn.net/u012122743/article/details/40740529
                        在8086下，cpu寄存器位数只有16位，而地址总线却有20位，为了能让16位的寄存器能够访问20位的地址空间，
                        所以使用CS、DS、ES、SS等寄存器来保存程序的段首地址。
                        在现在的32位cpu上，虽然用任何一个通用寄存器来间接寻址，不用分段就可以访问4G空间中任意的内存地址。
                        但这并不意味着，此时段寄存器就不再有用了，在保护模式下，一个地址空间是否可以被写入，可以被多少优先级的代码写入等问题，
                        仍需要通过分段的手段来解决，所以仍有一个16位的段寄存器用以协助段寻址。
                        在这种模式下，在内存的特定位置，会存放一个“段描述符表”，表中的每一项都记录了某一段的段首地址、段界限、段属性。
                        可称“段描述符表”的各项为“段选择子”，16为的段寄存器负责索引到“段选择子”。
                        根据段选择子得到对应的段地址后，再根据段内偏移量，就能极端得到线性地址，继而转换得到物理地址。
                        另外cpu通过两个新的寄存器GDT/LDT，来记录“段描述符表”的具体地址。
                        GDT属于系统，LDT属于程序，同一计算机上的所有程序公用一个LDT。祥参：https://www.jianshu.com/p/af0d8c44fd6b
            内存管理机制
                分段机制
                    四个基本组成部分
                        逻辑地址
                            段寄存器地址 + 偏移量（有效地址）
                        段选择寄存器
                            存放段选择符的寄存器：CS、DS、SS
                            2字节： 
                                0~1位：权限级别（RPL） ring 0~3
                                第2位：TI，为0，表索引GDT，为1，表索引LDT
                                3~15位：索引
                        段描述符
                            描述段的基地址、长度、各种属性（读写、访问权限）
                            参：file://段描述符.jpg
                        段描述符表
                    通过逻辑地址访问内存时，cpu通过逻辑地中的段选择符，索引到
                    段描述符表中的相应项，然后检测访问是否合法，最后转为线性地址
        2.3 x86的基本运行规则
            三种基本模式
                实模式
                    在该模式下，逻辑地址转换后，即为物理地址
                    cpu可以访问1M+64KB的物理地址空间
                    操作系统或BIOS通常在改模式下准备必要的数据结构，初始化关键的寄存器，
                    然后切换到保护模式
                保护模式
                    可以访问架构所允许的所有地址空间
                    本章中的所有讲解，如果特别说明，都是运行在保护模式下的
                虚拟8086模式
                    为了兼容早起的8086程序
                    使这些程序在执行时，无需真正的从保护模式切换到实模式
            基本寄存器组
                通用寄存器： 8个32位通用寄存器，如EAX、EDX等，用来保存临时变量、栈指针等
                    EAX：通用寄存器。
                        相对其他寄存器，在进行运算方面比较常用。 
                        在保护模式中，也可以作为内存偏移指针（此时，DS作为段 寄存器或选择器） 
                    EBX：通用寄存器。
                        通常作为内存偏移指针使用（相对于EAX、 ECX、EDX），
                        DS是默认的段寄存器或选择器。 在保护模式中，同样可以起这个作用。 
                    ECX：通用寄存器。
                        通常用于特定指令的计数。
                        在保护模式中， 也可以作为内存偏移指针（此时，DS作为 寄存器或段选择器）。
                    EDX：通用寄存器。
                        在某些运算中作为EAX的溢出寄存器（ 例如乘、除）。
                        在保护模式中，也可以作为内存偏移指针（此时， DS作为段 寄存器或选择器）
                内存管理寄存器：包括段寄存器和描述符表寄存器
                    CS 代码段，或代码选择器。
                        同IP寄存器(稍后介绍) 一同指向当前正在执行的那个地址。
                        处理器执行时从这个寄存器指向的段（实模式）或内存（保护模式）中获取指令。
                        除了跳转或其他分支指令之外， 你无法修改这个寄存器的内容。 
                    DS 数据段，或数据选择器。
                        这个寄存器的低16 bit连同ESI一同指向的指令将要处理的内存。
                        同时， 所有的内存操作指令 默认情况下都用它指定操作段(实模式)
                        或内存(作为选择器
                        在保护模式，这个寄存器可以被装入任意数值， 然而在这么做的时候需要小心一些。
                        方法是，首先把数据送给AX， 然后再把它从AX传送给DS(当然，也可以通过堆栈来做)
                    ES 附加段，或附加选择器。
                        这个寄存器的低16 bit连同EDI一同指向的指令将要处理的内存。
                        同样的， 这个寄存器可以被装入任意数值，方法和DS类似。 
                    FS F段或F选择器(推测F可能是Free?)。
                        可以用这个寄存器作为默认段寄存器或选择器的一个替代品。 
                        它可以被装入任何数值，方法和DS类似。 
                    GS G段或G选择器(G的意义和F一样， 没有在Intel的文档中解释)。
                        它和FS几乎完全一样。 
                    SS 堆栈段或堆栈选择器。
                        这个寄存器的低16 bit连同ESP一同指向下一次堆栈操作(push和pop) 
                        所要使用的堆栈地址。这个寄存器也可以被装入任意数值，
                        你可以通过入栈和出栈操作来给他赋值， 
                        不过由于堆栈对于很多操作有很重要的意义，
                        因此， 不正确的修改有可能造成对堆栈的破坏。
                标志寄存器：EFLAGS寄存器，保存程序运行中的一些标志位信息
                    追踪标志TF(Trap Flag)
                        当追踪标志TF被置为1时，CPU进入单步执行方式，
                        即每执行一条指令，产生一个单步中断请求
                    方向标志DF(Direction Flag)
                    中断允许标志IF(Interrupt-enable Flag)
                    I/O特权标志IOPL
                        I/O特权标志用两位二进制位来表示，也称为I/O特权级字段。
                        该字段指定了要求执行I/O指令的特权级。
                        如果当前的特权级别在数值上小于等于IOPL的值，
                        那么，该I/O指令可执行，否则将发生一个保护异常。
                    嵌套任务标志NT
                    重启动标志RF
                    虚拟8086方式标志VM
                程序指针寄存器：PC寄存器(EIP寄存器)
                    ESI：通常在内存操作指令中作为“源地址指针”使用。
                        当然， ESI可以被装入任意的数值， 但通常没有人把它当作通用寄存器来用。 
                        DS是默认段寄存器或选择器。 
                    EDI：通常在内存操作指令中作为“目的地址指针”使用。
                        当然， EDI也可以被装入任意的数值， 但通常没有人把它当作通用寄存器来用。 
                        DS是默认段寄存器或选择器。 
                    EBP：这也是一个作为指针的寄存器。
                        通常， 它被高级语言编译器用以建造‘堆栈帧’ 来保存函数或过程的局部变量，
                        不过，还是那句话， 你可以在其中保存你希望的任何数据。 
                        SS是它的默认段寄存器或选择器。
                浮点运算寄存器：浮现运算协处理器
                控制寄存器：5个控制寄存器（CR0~CR4）,决定CPU的运行模式和特征等
                    CR0的作用是切换实模式和保护模式
                    cr0: 存储了CPU控制标记和工作状态
                        PG: 是否启用内存分页
                        AM: 是否启用内存对齐自动检查
                        WP: 是否开启内存写保护，
                            若开启，对只读页面尝试写入时将触发异常，
                            这一机制常常被用来实现写时复制功能
                        PE: 是否开启保护模式
                    cr1: 保留未使用
                    cr2: 页错误出现时保存导致出错的地址
                    cr3: 存储了当前进程的虚拟地址空间的重要信息——页目录地址
                        保存了当前进程所使用的虚拟地址空间的页目录地址，
                        可以说是整个虚拟地址翻译中的顶级指挥棒，
                        在进程空间切换的时候，CR3也将同步切换
                    cr4: 也存储了CPU工作相关以及当前人任务的一些信息
                    cr8: 64位新增扩展使用
                其他寄存器：调试寄存器(DR0~DR7)、内存区域类型寄存器、性能监控寄存器等
            权限控制
                段保护
                页保护
                    在页表项中引入一个User/Supervisor位
                    当程序运行在CPL=0、1、2时，为Supervisor模式，可访问所有页面
                    运行在CPL=3时，为User模式，只能访问User页面
        2.4 中断与异常
            中断
                中断给外部设备提供了一种“打断cpu当前任务，并响应外设”的手段
                中断控制器
                    外设的中断信号不是直接发给cpu的，而是经过中断控制器（PIC/APIC）转发到CPU
                    PIC （Programmable Interrupt controller）
                        有IR0~IR8共8个引脚，可接8个外设，IR0优先级最高，IR7优先级最低。
                        外设产生中断时，IRn上产生中断电平信号，然后PIC内个有个中断屏蔽寄存器
                        （IMR，可以屏蔽中断），如果PIC没有屏蔽该中断，PIC就设置自身的IRR(/IRQ)
                        （中断请求寄存器，8位，对应8个中断引脚）的相应位，并通过CPU的INT引脚，
                        告知CPU有中断来了。
                        另一方面，当CPU响应某次中断（准备跳转）后，
                        就通过INTA引脚告知PIC已经处理了某个（当前最高优先级的）中断了，
                        于是PIC就把IRR中具有最高优先级的位清零，并设置PIC内部的“服务中寄存器”
                        （ISR，8位）的相应位。然后CPU还会向INTA管脚再发一次信号，
                        PIC收到这个信号后，计算其处理的那个中断对应的“中断向量”，将之提交到数据总线上
                        CPU拿到这个中断向量后，跳转到相应的中断服务代码中，并发送EIO信号给PIC
                        PIC清除ISR中相应的位，表明CPU已经完成中断跳转，进入到相应的内核中了
                    APIC （Advanced Programmable Interrupt controller）
                        PIC只适用于单CPU平台，不适用于多CPU平台，APIC就是为解决该问题而产生的
                        APIC由cpu中的LAPIC（Local APIC）和南桥中的IOAPIC（I/O APIC）组成
                        IOAPIC通常有24个具有不同优先级的管脚，用以连接外部设备
                        收到某一管脚的中断后，IOAPIC根据系统设定的重映射表(PRT)，找到管脚对应的表项，
                        再根据该表项中的各个字段信息，得到一条包含该中断所有信息的中断消息，
                        在通过系统总线，送给LAPIC，LAPIC择机将该消息提交给CPU
                        在LAPIC内部，也有PIC功能相似的IRR、ISR、EOI等寄存器
                        其中IRR、ISR有256位，EOI有32位
                        另外，CPU可通过LAPIC的ICR（中断命令寄存器）向指定的一个/多个cpu发送中断消息
                中断的分类
                    外部中断
                        IOAPIC来的中断、处理器间中断
                    可屏蔽中断
                        可以通过某种方式进行屏蔽的中断，对应的是不可屏蔽中断
                    软件产生的中断
                        通过INT n 指令产生的中断
                    外部中断通常是可屏蔽中断（但不绝对）
                中断的优先级
                    PIC有8个优先级
                    APIC中，IOAPIC的管脚不再有优先级的概念，而是根据管脚对应的重映射表(PRT)项中的
                    vector字段决定，vecotr是x86架构用以索引IDT表的下表，范围从0~255，
                    值越大，优先级越高，其中32~255供外部中断使用
                中断的屏蔽
                    中断寄存器收到中断后，并不一定马上交给CPU，因为CPU可能正处于中断处理中或其他情况
                    而屏蔽了全部或更低优先级的中断，这是中断寄存器会暂存该中断，等待机会将中断提交给CPU
                    cpu可通过一下方法屏蔽中断
                        CLI/STI指令 （Clear Interrupt, Set Interrupt）
                            操作系统最常用的方法，CLI把cpu的标志寄存器的IF位置0，阻止接收中断
                        TPR寄存器
                            根据该寄存器的优先级，部分屏蔽外部中断
                        PIC/IOAPIC的中断屏蔽位
                            该方法会把中断直接忽略
                IDT表 （Interrupt Discriptor Table）
                    IDT表中存放通过各种中断和异常处理函数的入口地址
                    当中断或异常发生时，CPU通过他们对应的“向量”索引IDT表，从而进到相应的处理函数
                    每个向量占8字节，x86最多有256个向量，故IDT表的最大长度为 8*256 = 2048 字节。
                    IDT表的基地址存在IDTR寄存器中。
                    系统将向量0~向量19（20个）预留给各种异常。
            异常
                和中断相比，异常最大的不同在于，它是程序（而非外部设备）触发的
                异常根据产生的原因和严重程序，可分为如下三类：
                    错误：一般可以被错误处理程序纠正，如缺页错误
                    陷阱：引发异常，如INT 80
                    终止：严重的，不可恢复的错误，导致程序终止的异常
            中断和陷阱的唯一区别是中断跳转后，EFLAGS寄存器的IF位会自动清零，中断关闭。
            中断和异常的处理流程
                1. cpu通过向量，索引IDT表，得到处理函数的地址
                2. 跳转到处理函数，执行处理函数
                    1). 保存被打断任务的上下文
                    2). 如果是中断，处理完后，要写EOI寄存器应答，伪中断和异常不需要
                    3). 恢复被打断任务的上下文，准备返回
                3. 从处理函数返回，恢复执行被打断的任务
        2.5 进程
            略
        2.6 I/O架构
            设备通常通过寄存器和设备RAM，将自己的功能呈现给CPU
            x86架构
                几种访问设备的方式
                    通过端口I/O访问设备
                        x86共有2^16(64K)个8位的I/O端口（地址0x0~0xFFFF）
                        2个或4个连续的8位I/O端口，可以组成16位或32位的I/O端口
                        I/O端口地址空间并不是线性地址空间或物理地址空间的一部分
                        CPU通过特殊的管脚表明这是I/O地址
                        #这种方式访问设备的寄存器还可以，如果访问设备的内存，
                        #因为I/O指令的地址空间只有64k，所以不适用访问的RAM
                    内存映射I/O访问设备（MMIO）
                        这种方式把设备的寄存器和RAM，映射到物理地址空间的某段地址
                        但注意与普通物理地址到线性地址的映射不同，
                        这里的"内存页"是不可被高速缓存的
                    DMA
                        DMA(直接内存访问)是一种CPU辅助设备，通过这种方式，不再需要CPU直接访问设备内存了
                        一种方式是CPU把要读/写的内存地址告诉给DMA后，
                        CPU主动通知DMA负责内存和设备内存之间数据的转移（转移完成后，通过中断通知CPU）
                        另一种方式是外围设备通知DMA，进行内存和设备内存之间数据的转移
                        （转移完成后，通过中断通知CPU）
                    IOMMU
                        后面讲VT-d时介绍
                PCI总线
                    Peripheral Component Interconnect，“外围器件互联”
                    PCI总线是一种树型结构，北桥中HOST-PCI桥是树根
                    PCI总线上可以挂接PCI设备和PCI桥（如ISA-PCI桥）
                    如图：file://PCI总线结构.jpg | file://PCI组织结构.jpg
                    读写操作只能在主从设备之间进行(从设备相互之间不能通信)，
                    从设备之间的数据交换需要通过主设备中转
                    与HOST-PCI相连的总线，称为总线0，其他层次总线的编号，是在BIOS枚举设备时确定的
                    设备标识符（BDF）：设备在PCI总线上的地址格式
                        总线编号(8bit)+某条总线上的设备编号(5bit)+某个设备上的子设备编号(3bit)
                        树根HOST-PCI上的总线被称为总线0
                        总线地址缩写为BDF（Bus+Device+Function）
                    PIC设备的配置空间
                        所谓的配置空间，按字面意思，就是对外围设备的属性进行配置时，可访问的地址范围
                        PCI规定设备的控制空间最多占256个字节，并对前64(0x0~0x3F)字节的格式进行统一规范。
                        如图：file://PCI配置空间.png
                        PCI Bar(PCI Base Address Registers, 基址寄存器)
                            参：https://www.cnblogs.com/zszmhd/archive/2012/05/08/2490105.html
                            参： https://blog.csdn.net/kunkliu/article/details/94380357
                            PCI配置空间中的0x10~0x27(24字节)对应了6个BAR寄存器（设备的基本地址寄存器）
                            Bar寄存器结构：
                                I/O端口方式：  26位地址空间+1位可预读标记+2位设备类型标记+1位I/O端口方式标记位
                                内存映射方式： 28位地址空间+1位保留位+1位内存映射方式标记位
                                ● Bar记录了设备寄存器或设备RAM在物理地址空间或I/O端口地址空间中的起始地址
                                （该地址是由BIOS或操作系统在枚举设备时，根据设备数量，按固定算法动态配置的）
                                该Bar中的地址是由BIOS或操作系统在电脑启动后自动设置的。
                                大部分体系结构中，memory address和I/O address都是分别编址的，
                                且使用不同的寻址指令，构成了两套地址空间，
                                也有少数体系结构将memory address和I/O address统一编址(如ARM)
                                北桥芯片负责地址的路由工作，它内部有一张address map，
                                记录了memory address，I/O address的映射信息
                                如图： file://设备地址映射关系表.png
                                ● Bar的最后1位区别当前表示的地址是memory address还是I/O address
                                （最后一个bit为1，标志使用I/O端口方式，为0，标志使用内存映射方式）
                                memory address和I/O address是共用一套地址总线，但通过控制总线上的信号区别
                                当前地址总线上的地址是memory address还是I/O address。
                                ● Bar的第3位标识该区域是否可预读
                                （通常RAM是可预读的，而设备的寄存器是不可预读的）
                            为什么会有6个Bar？
                                windows系统/设备管理器/显卡设备/资源，参：file://显卡使用的资源.jpg
                                可以看到它同时使用了IO映射和内存映射，并分别映射到多个内存地址范围
                        中断引脚
                            1字节（0x3B处），表示设备链接的是哪个中断引脚（INTA/INTB/INTC/INTD）
                        中断线
                            该寄存器不起控制作用，只起到一个保存作用，
                            BIOS通常用它来保存设备所连的PIC/IOAPIC的管脚号？？
                        枚举PCI设备
                            参图：file://PCI组织结构.jpg
                            PCI设备的枚举和资源分配，是有BIOS或系统在启动的时候分配的，
                            如果是BIOS提供的，那么BIOS还会向系统再提供PCI设备枚举接口（称为PICBIOS）
                            PCI桥
                                枚举的关键是发现PCI桥
                                PCI桥也是PCI设备，也有配置空间，参图：file://PCI配置空间.png
                                配置空间中的 Header Type 字段如果为1，表明该设备是桥设备
                                由图可见，一个PCI桥，可以让一条(上级)总线，产生一条(下级)子总线
                                PCI桥主要有三个属性：上级总线号，(直接)下级总线号，最大下级总线号
                            枚举是从HOST-PIC桥（位于北桥芯片中）开始的，该桥连接的子总线位0号总线
                            总线下面可以直接挂最多32个设备（包括桥设备）
                            枚举到的每个设备，不过是PCI设备，还是桥设备，都会分配一个设备号，
                            还会为PCI设备继续分配子设备号，
                            而当发现总线0下面的第一个桥设备后，那该桥设备的直接下级总线号就是1
                            然后再枚举总线1上的各个PCI设备，如果总线1上发现了一个桥设备，
                            则该发现的桥设备的直接下级总线就是总线2，然后再枚举总线2上的PCI设备
                            由此可发现，遍历桥设备使用的是深度优先查找法
                            遍历完成之后，每个PCI设备的总线号、设备号、子设备号（BDF）就定下来了
                            x86架构使用IO端口地址空间中的0xCF8~0xCFF，共8个寄存器，完成PCI访问
                                0xCF8,0xCF9 : 存放BDF（设备地址）
                                0xCFA,0xCFB : 存放要访问的设备的配置空间的偏移量（猜测，不确定）
                                0xCFC~0xCFF : 存放读取到的结果
                PCIE总线
                    参：file://PCIE总线结构.jpg
                    PCIE定义了技术数据包的通信协议，包括物理层、数据链路层、事务层
                    在事务层中，PCIE定义了内存读写、IO读写、配置空间读写、消息事务
                    PCIE将PCI总线的配置空间大小，从256字节，扩展到4KB
                    PCIE还增加了一种新的MMIO方法用以访问扩充过的配置空间
                    DMA重映射可以利用PCIE的内存读写数据包，根据包中的设备标识符(BDF)
                    和地址信息，为每个设备提供独立的地址转换
                    https://blog.csdn.net/yingqiangli/article/details/109161667
                    PCI采用的是总线型拓扑结构，而PCIe则采用树形拓扑结构
                    在<file://imgs/PCIE总线结构.jpg>图中的设备介绍：
                        ● Root Complex(RC)内部一般实现了一条内部PCIe总线，
                          以及通过若干个PCIe bridge，扩展出一些PCIe Port
                        ● PCIe Endpoint(EP)，就是PCIe终端设备，比如PCIe SSD
                          Legacy Endpoint(EP)，接口是PCIe，
                          但是内部的行为却和传统的PCI或者PCI-x一样（比如支持IO空间）
                          这些Endpoint可以直接连在RC上，也可以通过Switch连到PCIe总线上
                        ● Switch用于扩展链路，提供更多的端口用以连接Endpoint
                          Switch有一个上游端口，和分出来的多个下游端口，
                          下游端口可以直接连接Endpoint，
                          也可以连接Switch，扩展出更多的PCIe端口
                          对每个Switch来说，它下面的Endpoint或者Switch，都是归他管的：
                          上游下来的数据，它需要甄别数据是传给它下面哪个设备，然后进行转发；
                          下面设备向RC传数据，也要通过Switch代为转发的。
                          因此，Switch的作用就是扩展PCIe端口，
                          并为挂在它上面的设备（endpoint 或者switch)提供路由和转发服务。
                    PCIe与采用总线共享式通讯方式的PCI不同，
                    PCIe采用点到点通讯方式，每个设备独享通道带宽，速度和效率都比PCI好
                    PCIe的三层结构
                        事务层
                            主要职责是创建/发送或者解析/接收TLP(Transaction Layer packet),
                            流量控制，QoS，事务排序等。
                        数据链路层
                            主要职责是创建/发送或者解析/接收DLLP(Data Link Layer packet)，
                            Ack/Nak协议（链路层检错和纠错），流控，电源管理等。
                        物理层
                            主要职责是处理所有的Packet数据物理传输，
                            发送端数据分发到各个Lane传输（stripe），
                            接收端把各个Lane上的数据汇总起来（De-stripe），
                            每个Lane上加扰（Scramble，）去扰（De-scramble)，
                            (加扰的目的是让0和1分布均匀，去除信道的电磁干扰EMI)
                            以及8/10或者128/130编码解码，等等
                    每个Endpoint都需要实现这三层，每个Switch的每个Port也是需要实现这三层的
                        参图：file://PCIe数据包的传输过程.png
                        Switch的主要功能是转发数据，为什么还需要实现事务层？
                        Switch必须实现这三层，因为数据的目的地信息是在TLP中的，
                        如果不实现这一层，就无法知道目的地址，也就无法实现数据寻址路由
                    四种TLP请求类型
                        Memory
                            这种TLP是我们最常见的
                        IO
                            新的PCIe设备（区别于Legacy PCIe设备）只支持内存映射，
                            之所以还存在访问IO空间的TLP，完全是为了照顾那些老设备
                        Configuration
                            所有的配置空间（Configuration）的访问，
                            都是Host发起的，确切的说是RC发起的，
                            往往只在上电枚举和配置阶段会发起Configuration的访问
                        Message
                            只有有中断，或者有错误等情况下，才会有Message TLP
                        总结
                            前三种分别用于访问内存空间、IO空间、配置空间
                            这三种请求在PCI或者PCI-X时代就有了
                            最后的Message请求是PCIe新加的，负责传输中断信息、错误信息等
                    Non-Posted和Posted
                        这四种请求，如果需要对方响应的，我们叫做Non-Posted的TLP；
                        如果不期望对方给响应的，我们称之为Posted TLP
                        对Non-Posted的Request，是一定需要对方响应的，
                        对方是通过返回一个Completion TLP来作为响应的
                        所以，按上面的方式，TLP又可分为 Request TLP 和 Completion TLP 两种
                        Configuration、IO访问，都是Non-posted的
                        Message TLP是Posted
                        Memory Read是Non-posted的，Memory Write是Posted
                        数据链路层提供了ACK/NAK机制，一定程度上能保证TLP正确交互，
                        因此能很大程度减小数据写失败的可能
                    各种TLP包及其简称
                        TLP包类型                                   简称
                        Memory Read                                 MRd
                        Memory Write                                MWr
                        Configuration Read（Type 0 and Type 1）     CfgRd0, CfgRd1
                        Configuration Write（Type 0 and Type 1）    CfgWr0, CfgWr1
                        Message Request with Data                   MsgD
                        Message Request without Data                Msg
                        Completion with Data                        CplD
                        Completion without Data                     Cpl
                    一个TLP，最多只能携带4KB有效数据
                    TLP结构
                        TLP主要由三部分组成：Header，Data Payload, CRC
                        Data Payload域，用以放有效载荷数据。
                        该域不是必须的，因为并不是每个TLP都必须携带数据的，
                        比如Memory Read TLP，它只是一个请求
                        一个TLP最大载重是4KB，数据长度大于4KB的话，就需要分几个TLP传输
                        ECRC(End to End CRC)域，它对之前的Header和Data生成一个CRC
                        它也是可选的，可以设置不加CRC
                        Header
                            参：file://PCIe TLP Header.png
                            Fmt：Format, 表明该TLP是否带有数据，Header长度是3DW还是4DW
                                Configuration和Completion 的TLP，其Header大小总是3字节；
                                Message TLP的Header总是4字节；
                                而Memory相关的TLP取决于地址空间的大小，
                                地址空间小于4GB的，Header大小为3DW，
                                大于4GB的，Header大小则为4DW
                            Type：TLP类型（占用5bit）
                            R： Reserved，为0；
                            TC: Traffic Class，优先级控制
                                TLP也分三六九等，优先级高的先得到服务。
                                这里是3比特，说明可以分为8个等级，0-7，
                                TC默认是0，数字越大，优先级越高；
                            Attr: Attrbiute, 属性，前后共三个bit，先不说；
                            TH: TLP Processing Hints，先不说；
                            TD: TLP Digest，控制是否使用CRC校验，
                                如果这个bit置1，说明该TLP包含ECRC，接收端应该做CRC校验；
                            EP: Poisoned data, 有毒的数据，远离；
                            AT: Address Type，地址种类，先不说；
                            Length： Payload数据长度，10个bit，最大1024，单位DW，
                                所以TLP最大数据长度是4KB; 该长度总是DW的整数倍，
                                如果TLP的数据不是DW的整数倍（不是4Byte的整数倍），
                                则需要用到下面两个域：
                                Last DW BE 和 1st DW BE。
                            Memory TLP
                              内存地址（目标地址）
                                对一个PCIe设备来说，它开放给Host访问的设备空间首先会
                                映射到Host的内存空间，Host如果想访问设备的某个空间，
                                TLP Header当中的地址应该设置为该访问空间在Host内存的
                                映射地址。如果Host内存空间小于4GB，则Memory读写TLP的
                                Header大小为3DW，大于4GB，则为4DW。那是因为，对4GB内存空间，
                                32bit的地址用1DW就可以表示，该地址位于Byte8-11；
                                而4GB以上的内存空间，需要2DW表示地址，该地址位于Byte8-15。
                              源地址
                                源是通过”Requester ID”告知
                                每个设备在PCIe系统中都有唯一的ID（BDF），
                                该ID由总线(Bus)、设备(Device)、功能(Function)三者唯一确定
                            Configuration TLP
                                Endpoint和Switch的配置（Configuration）格式不一样，
                                分别为Type 0和 Type 1来表示
                                配置可以认为是一个Endpoint或者Switch的一个标准空间，
                                这段空间在初始化时也需要映射到Host的内存空间
                                Host访问PCIe设备的配置空间，只需指定目标设备的ID就可以了，
                                不需要内存地址
                                图中Ext Reg Number + Register Number相当于配置空间的偏移
                            Message TLP
                                用以传输中断、错误、电源管理等信息
                                取代PCI时代的边带信号传输
                                图中Message Code来指定该Message的类型
                                不同的Message Code，最后两个DW的意义也不同，这里不展开
                            Completion TLP
                                non-posted request TLP，才有Completion TLP
                                Requester 的TLP当中都有Requester ID和Tag，
                                来告诉接收者发起者是谁。
                                那么响应者的目标地址就很简单，照抄发起者的源地址就可以了
                                Completion TLP，一方面，可以返回请求者的数据，
                                比如作为Memory或者Configuration Read的响应；
                                另一方面，还可以返回该事务（Transaction）的状态，
                                因此，在Completion TLP的Header里面有一个Completion Status，
                                用以返回事务状态
                    配置和地址空间
                        PCI或者PCI-X时代就有配置空间的概念
                        PCIe整个配置空间由256 Bytes扩展成4KB，前面256 Bytes保持不变
                        参图：file://PCIe配置空间的前64字节.jpg
                        对Endpoint Configuration（Type 0），提供了最多6个BAR，
                        而对Switch（Type 1）来说，只有2个
                        BAR是干什么用的
                            每个PCIe设备，都有自己的内部空间，
                            这部分空间如果开放给Host（软件或者CPU)访问，
                            那么Host怎样才能往这部分空间写入数据，或者读数据呢
                            CPU只能直接访问Host内存（Memory）空间
                            （或者IO空间，我们不考虑）
                            不能亲自跟那些PCIe外设打交道,而是交由RootComplex去办
                            如果CPU想读PCIe外设的数据，先叫RC通过TLP把数据
                            从PCIe外设读到Host内存，然后CPU从Host内存读数据；
                            如果CPU要往外设写数据，则先把数据在内存中准备好，
                            然后叫RC通过TLP写入到PCIe设备
                            具体实现就是上电的时候，系统把PCIe设备开放的空间
                            （系统软件可见）映射到内存空间
                            CPU要访问该PCIe设备空间，只需访问对应的内存空间
                            RC检查该内存地址，如果发现该内存空间地址是某个
                            PCIe设备空间的映射，就会触发其产生TLP，
                            去访问对应的PCIe设备，读取或者写入PCIe设备
                            一个PCIe设备，可能有若干个内部空间需要映射到内存空间
                            设备出厂时，这些空间的大小和属性都写在
                            Configuration BAR寄存器里面
                            然后上电后，系统软件读取这些BAR，
                            分别为其分配对应的系统内存空间，
                            并把相应的内存基地址写回到BAR
                            '''BAR的地址其实是PCI总线域的地址，
                            CPU访问的是存储器域的地址，CPU访问PCIe设备时，
                            需要把总线域地址转换成存储器域的地址。'''
                            系统软件为PCIe分配映射空间的例子
                                上电时，系统软件首先会读取PCIe设备的BAR0
                                然后系统软件往该BAR0写入全1
                                BAR寄存器有些bit是只读的，
                                是PCIe设备在出厂前就固定好的bit，
                                写全1进去，如果值保持不变，
                                就说明这些bit是厂家固化好的，
                                这些固化好的bit提供了这块内部空间的一些信息
                                低12没变，表明该设备空间大小是4KB(2^12)
                                (注意低12位中的低4位中，可能存在本来就为1的位
                                所以主要是根据这低12位中的高8位来判断变化的)
                                低4位表明了该存储空间的一些属性(IO映射还是
                                内存映射，32bit地址还是64bit地址，能否预取)
                                然后系统软件根据这些信息，在系统内存空间找到
                                这样一块地方来映射这4KB的空间，
                                把分配的基地址写入到BAR0
                                一个PCIe设备可能有若干个内部空间需要开放出来，
                                系统软件依次读取BAR1，BAR2。。。，直到BAR5，
                                完成所有内部空间的映射
                        前面说每个PCIe设备都有一个配置空间，其实这样说是不准确的，
                        而是每个PCIe设备至少有一个配置空间。
                        一个PCIe设备，它可能具有多个功能（function），
                        比如既能当硬盘，还能当网卡。每个功能对应一个配置空间。
                        一个PCIe系统，可以最多有256条Bus，
                        每条Bus上可以挂最多32个Device，
                        而每个Device最多又能实现8个Function，
                        而每个Function对应着4KB的配置空间
                        上电的时候，这些配置空间都是需要映射到Host的内存空间，
                        因此，需要占用内存空间是：256*32*8*4KB =256MB
                        系统软件是如何读取Configuration空间呢？
                        不能通过BAR中的地址，为什么？
                        别忘了BAR是在Configuration中的，
                        你首先要读取Configuration，才能得到BAR。
                        前面不是系统为所有可能的Configuration预留了256MB内存空间吗？
                        系统软件想访问哪个Configuration，
                        只需指定相应Function对应的内存空间地址，
                        RC发现这个地址是Configuration映射空间，
                        就会产生相应的Configuration Read TLP
                        去获得相应Function的Configuration。
                        结束前，强调一下，只有RC才能发起Configuration的访问请求，
                        其他设备是不允许对别的设备进行Configuration读写的
                        Switch的Configuration
                            参：file://imgs/PCIe switch的配置空间.jpg
                            Switch有一个上游端口（靠近RC）和若干个下游端口，
                            每个端口其实是一个Bridge，都是有一个Configuration的
                            每个Configuration描述了其下面连接设备空间映射的范围，
                            分别由Memory Base和Memory Limit来表示。
                            对上游端口，其Configuration描述的地址范围是
                            它下游所有设备的映射空间范围，
                            而对每个下游端口的Configuration，
                            描述了连接它端口设备的映射空间范围
                            参图：file://imgs/PCIe Switch记录设备地址范围.jpg
                    TLP的路由
                        PCIe共有三种路由方式：
                        基于地址（Address）路由，
                        基于设备ID（BDF）路由，
                        还有就是隐式（Implicit）路由
                        不同类型的TLP，其寻址方式也不同
                            TLP类型                   路由方式
                            Memory Read TLP           地址路由
                            Memory Write TLP          地址路由
                            Configuration Read TLP    ID路由
                            Configuration Write TLP   ID路由
                            Completion TLP            ID路由
                            Message TLP               都行
                        地址路由
                            当一个Endpoint收到一个Memory Read或者Memory Write TLP，
                            它会把TLP Header中的地址跟它Configuration当中的
                            所有BAR寄存器比较，
                            如果TLP Header中的地址落在这些BAR的地址空间，
                            那么它就认为该TLP是发给它的，于是接收该TLP，否则就忽略。
                            当Switch上游端口收到一个Memory Read或Memory Write TLP
                            它首先把TLP Header中的地址跟它自己Configuration当中的
                            所有BAR寄存器比较，
                            如果TLP Header当中的地址落在这些BAR的地址空间，
                            那么它就认为该TLP是发给它的，
                            于是接收该TLP（这个过程与Endpoint的处理方式一样）；
                            如果不是，然后看这个地址是否落在其下游设备的地址范围内
                            （是否在memory base 和memory limit之间），
                            如果是，说明该TLP是发给它下游设备的，因此它要完成路由转发；
                            如果地址不落在下游设备的地方范围内，
                            说明该TLP不是发给它下面设备的，因此不接受该TLP。
                            如果是TLP从下游往上走呢？
                            Switch首先把TLP Header中的地址跟它自己Configuration当中的
                            所有BAR寄存器比较，
                            如果TLP Header当中的地址落在这些BAR的地址空间，
                            那么它就认为该TLP是发给它的，
                            于是接收该TLP（跟前面描述一样）；
                            如果不是，然后看这个地址是否落在其下游设备的地址范围内
                            （是否在memory base 和memory limit之间），
                            如果是，这个时候不是接受，而是拒绝；
                            相反，如果地址不落在下游设备的地方范围内，
                            Switch则把该TLP传上去
                        ID路由
                            在一个PCIe拓扑结构中，由ID(BDF)能唯一找到某个设备的某个功能
                            这种按设备ID号来寻址的方式叫做ID路由
                            Configuration TLP和Completion TLP（以C打头的TLP）按ID路由，
                            Message在某些情况下也是ID路由
                            当一个Endpoint收到一个这样的TLP，
                            它用自己的ID和收到TLP Header中的BDF比较，
                            如果是给自己的，就收下TLP，否则就拒绝
                            如果是一个Switch收到这样的一个TLP
                            参：file://imgs/PCIe switch的配置空间.jpg
                            可发现这三个寄存器： Subordinate Bus Number，
                            Secondary Bus Number和Primary Bus Number
                            他们的关系参图：file://imgs/unnamed_1.png
                            当一个Switch收到一个基于ID寻址的TLP，
                            首先检查TLP中的BDF是否与自己的ID匹配，
                            如匹配，说明该TLP是给自己的，收下；
                            否则，则检查该TLP中的Bus Number是否落在
                            Secondary Bus Number和Subordinate Bus Number之间
                            果是，说明该TLP是发给其下游设备的，
                            然后转发到对应的下游端口；如果其他情况，则拒绝这些TLP。
                        隐式路由
                            只有Message TLP才支持隐式路由
                            在PCIe总线中，有些Message是与RC通信的
                            RC是该TLP的发送者或者接收者，
                            因此没有必要明明白白的指定地址或者ID
                            Message TLP还支持地址路由和ID路由，但以隐式路由为主
                从插口上区分PCI，PCIe
                    参：file://imgs/PCI和PCIe插口的区别.jpg
                    PCI插槽都是等长的，防呆口位置靠上
                    PCIe插槽大大小小，有x1，x2，x4，x8，
                    x12，x16和x32共计7种版本，对应1/2/4/8/12/16/32通道，
                    其中PCI-E x32由于体积问题，仅使用在某些特殊场合中，
                    对应的量产产品几乎为零，PCI-E x12则主要用在服务器领域，
                    基本不会出现在消费级平台上。
                    还可以通过主板上的标注来分辨主板的PCI-E插槽标准
                    Q: 我主板上没有x1的插槽，
                       我x1的串口卡能不能插在x4的插槽里。
                    A: 可以，完全没有问题。
                       除了有点浪费外，串口卡也将以x1的方式工作
                    Q: 我主板上只有一个x16的插槽，被我的显卡占据了。
                       我还有个x16的RAID卡可以插在x8的插槽内吗？
                    A: 可以！你的RAID卡将以x8的方式工作
                    Q: 我的显卡是PCIe 3.0的，主板是PCIe2.0的，能工作吗？
                    A: 可以，会以2.0工作。反之，亦然
                    Q: 我把x16的显卡插在主板上最长的x16插槽中，
                       可是benchmark下来却说跑在x8下，怎么回事?！
                    A: 主板插槽x16不见得就连在支持x16的root port上，
                       最好详细看看主板说明书，有些主板实际上是x8
                    Q: 我新买的SSD是Mini PCIe的，Mini PCIe是什么鬼
                    A: Mini PCIe接口常见于笔记本中，为54pin的插槽。
                       多用于连接wifi网卡和SSD,注意不要和mSATA弄混了，
                       两者完全可以互插，但大多数情况下不能混用
        2.7 时钟
            根据工作方式不同，可分为两类：
                周期性时钟
                单次计时时钟
            x86常用的时钟
                PIT（programmable interrupt timer）
                    频率1000Hz左右，通常接中断控制器的IRQ0
                    PIT是一种低精度时钟，已渐渐被高精度时钟所代替
                RTC（real time clock）
                    通常和CMOS集成在一起，有CMOS供电
                    频率范围在2~8192Hz，通常接中断控制器的IRQ8
                    RTC常用作为操作系统提供日期时间
                TSC（time stamp counter）
                    与普通时钟不同，它是个单调递增的及数字（64位）
                    是频率与CPU频率相关
                    通过rdtsc指令，可以读取其值，不会产生中断
                LAPIC Timer
                    根据LAPIC总线（参上文的APIC中断）频率产生的时钟
                    由于LAPIC是每个CPU一个，所以其中断也是对本地cpu的
                    可以通过寄存器配置，最总线周期分频，产生不同频率的中断
                HPET（High Precision Event Timer）
                    最低频率10MHz，可提供最多8个时钟
                    搭配32个比较器+匹配器，又可配置成32个子时钟，
                    每个子时钟可以按不同频率产生中断
                    HPET用以替代传统的PIT和RTC，此时平台的IRQ0、IRQ8中断被HPET占用
                系统可以根据需要使用上述时钟中的一个或多个，
                使用多个时钟的一个明显缺点是，过多的时钟中断会影响系统的性能
            操作系统的时钟观
                从操作系统角度看，时钟的作用分为如下两类
                1. 提供统计值，及驱动事件
                    提供统计值，是指系统使用时钟来维护一些必要的数据
                    如一个进程在用户态/内核态的时间，系统的时间等
                    驱动时间是指驱动以时间为资源的程序，典型的就是进程
                2. 维护定时器
                    如内核为IO操作注册了超时定时器、应用程序中使用的定时器接口等
                需指出的是，系统往往会对时钟架构进行封装以方便维护和使用
    第三章  虚拟化概述
        3.1 可虚拟化架构与不可虚拟化架构
        3.2 处理器虚拟化
            3.2.1 指令的模拟
                从使用的角度看，物理处理器无非就是一些寄存器，
                并可以自动读取指令，对指定的寄存器进行操作
                操作结果会反映到特定的寄存器上。
        3.3 内存虚拟化
        3.4 I/O虚拟化
        3.5 VMM的功能和组成
        3.6 VMM的分类
        3.7 典型的虚拟化产品及其特点
    第四章  基于软件的完全虚拟化
        4.1 概述
        4.2 CPU虚拟化
        4.3 内存虚拟化
        4.4 I/O虚拟化
    第五章  硬件辅助虚拟化
        5.1 概述
        5.2 CPU虚拟化的硬件支持
        5.3 CPU虚拟化的实现
        5.4 中断虚拟化
        5.5 内存虚拟化
        5.6 I/O虚拟化的硬件支持
        5.7 I/O虚拟化的实现
        5.8 时间虚拟化
    第六章  类虚拟化技术
        6.1 概述
        6.2 类虚拟化体系结构
        6.3 Xen的原理与实现
        6.4 XenLinux的运行
CPU虚拟化
    软件实现
        二进制预翻译
    硬件实现：
        VT-x ：Virtual Technology for x86
        VT-x不仅需要处理器的支持，也需要主板、BOIS的支持
        1. 跟操作模式/非根操作模式 (统称VMX)
        非根操作模式时，特权指令=敏感指令
        2. VMCS ：virutal machine control structure
        让cpu自动的将上下文寄存器进行保存或载入 
            VMCS是保存在内存中的数据结构，每个对应一个虚拟CPU
            在任意时刻，有一个VMCS绑定在一个CPU上
            VSCS包含的信息
                客户机上下文（非根模式的 CPU状态）
                宿主机上下文（跟模式的CPU状态）
                VM-Entry控制域（对VM-Entry的过程进行控制）
                VM-Exit控制域（对VM-Exit的过程进行控制）
                VM-Execution控制域（控制处理器在非根模式下的行为）
                VM-Exit信息域（提供VM-Exit的原因和其它信息）
            可通过 VMXON/VMXOFF 控制是否打开cpu的VMCS功能
            VM-Entry、VM-Exit ：在根模式与非根模式之间切换
内存虚拟化
    软件实现
        VMM的虚拟MMU装载客户机维护的页表，
        但虚拟MMU不会把该页表直接交到物理MMU上
        而是把VMM维护的影子页表交到物理MMU上
    硬件辅助实现
        EPT : extended page table (EPT页表)
        CPU内部两步转换：GVA -> GPA，GPA -> HPA
        VMM的虚拟MMU把客户机的页表，直接交给真实MMU，
        真实MMU做完一次转换后，还会由EPT进行二次转换，
        EPT使用VMM维护的GPA转HPA的映射表进行二次转换。
IO虚拟化
    对设备访问的方式
        IO端口
            in/out 端口空间(2^16=64k)，直接操作物理设备
        MMIO
            将IO设备的RAM映射进内存区域(0xfxxxxxxx)，
            通过内存读写操作设备
        DMA
            使cpu从数据在IO和内存之间的转移工作中解脱出来
            先告诉DMA要进行数据转移的地址空间，
            再发起地址转移操作
        IOMMU
    软件虚拟化
        IO端口
            IO端口方式和MMIO方式，都可通过陷入的方式，被VMM截获
            对于IO端口方式，设备模型会在VMM中注册，客户机对VMM
            端口的访问，被VMM分发至设备模型
        MMIO
            对于MMIO方式， VMM维护客户机物理地址到真实物理
            地址的映射，VMM还维护客户机虚拟地址到真实物理地址的
            影子页表，VMM不会在影子页面中建立客户机MMIO相关的内存
            地址到真实物理地址（在影子页表中）的映射。
        DMA
            在DMA时，客户机需要告诉DMA要操作的客户机物理内存地址
            和设备内存地址，还要通知启动DMA操作，这些都是通过
            设备的寄存器来控制的，所以VMM都能拦截到
            如IDE控制器，客户端通过将其PCI配置空间中的BMIBA命令
            寄存器的第0位置1，即可发起DMA操作。
        PCI配置空间
    硬件虚拟化
        VT-d
            VT-d : Virtualization Technology for Directed I/O
            在北桥中增加'DMA重映射硬件'（IOMMU），
            按照https://blog.csdn.net/hx_op/article/details/104029622
            中的图2，该'DMA重映射硬件'位于RC（Root Complex）中,
            两者不冲突，因为RC就位于北桥中。
            客户机访问DMA时的地址，
            会被该硬件截获，转换成客户机相关的内存位置。
            转换的方式是：
                客户机向PDI设备的DMA发送BDF时，被北桥截获
                （注意PIC总线的根就在北桥上）
                北桥的DMA重映射硬件根据BDF的bus字段，索引
                '根条目表'，继而索引到该根关联的'上下文条目表',
                然后用BDF的[dev:func]索引该表中的特定项，
                根据索引到的上下文条目的ASR字段，找到该设备对应的
                I/O页表，I/O页表中记录了GPA到MPA的映射，
                于是DMA重映射硬件，就可以根据此表做地址转换了。
            DMA重映射注意事项
                参：https://blog.csdn.net/hx_op/article/details/104029622
                对于由PCIe switch扩展出的PCI桥及桥下设备，
                在发送DMA请求时，BDF是PCIe switch的，
                这样的话该PCI桥及桥下所有设备都会使用PCIe switch的
                BDF去定位Context Entry（上下文条目表），
                找到的表项也是同一个，如果将这个PCI桥下的不同设备分给不同虚机，
                由于会使用同一份页表，这样会产生问题，
                针对这种情况，当前PCI桥及桥下的所有设备必须分配给同一个虚机，
                这是VFIO中组的概念，后面会再讲到。
            VT-d的缓存
                为了提高检索速度，VT-d对上下文条目表提供了缓存功能
                称为IOTLB,由软件（调用接口）负责在必要的时候刷新该缓存
            VT-d的特点
                VT-d允许宿主机将某些硬件资源（比如硬盘、显卡、网卡）
                的管辖权直接移交给虚拟机，此时宿主机将不能再使用此硬件
                虚拟机会以'直通','独占'的方式使用它们
            访问IO端口或MMIO
                通过VT-x技术，客户机使用的是真实的CPU，
                而不是VMM提供的虚拟CPU，所以只要客户机能拿到真实
                的外设总线地址，可以直接访问该外设
            如何让DMA直接访问客户机的内存空间
                如PIC设备的DMA，就是在设备上的，
                所以不能指望DMA本身支持虚拟化
        单根虚拟化
            https://blog.51cto.com/maomaostyle/1439651
                SRIOV属于VT-d技术的一个分支
            I/O 虚拟化技术有三种：
                Device Emulation    软件模拟
                PCI Pass-through    直通，如VT-d    
                SR-IOV              本节所讲
            单根虚拟化(SR-IOV)是PCI Express (PCIe) 规范的扩展
            https://www.jianshu.com/p/a2fca5519d37
            SR-IOV （Single Root I/O Virtualization）
            SR-IOV 使得一个单一的功能单元能看起来像多个独立的物理设备
            一个带有 SR-IOV 功能的物理设备能被配置为多个功能单元
            PF（Physical Functions）：
                这是完整的带有 SR-IOV 能力的PCIe 设备。
                PF 能像普通 PCI 设备那样被发现、管理和配置。
            VF（Virtual Functions）：
                简单的 PCIe 功能，它只能处理 I/O。
                每个 VF 都是从 PF 中分离出来的。
                每个物理硬件都有一个 VF 数目的限制。
                一个 PF 能被虚拟成多个 VF 用于分配给多个虚拟机。
                https://www.cnblogs.com/jmilkfan-fanguiju/p/10589724.html
                    每个 PF 最多可有 64,000 个与其关联的 VF
                    缺省情况下，SR-IOV 功能处于禁用状态，
                    PF 充当传统 PCIe 设备
                    PF 可以通过寄存器创建 VF，一旦在 PF 中启用了 SR-IOV，
                    就可以通过 PF 的总线、设备和功能编号（路由 ID）
                    访问各个 VF 的 PCI 配置空间
                    创建 VF 后，用户可以直接将每个 VF 直接分配给虚拟机，
                    绕过虚拟机监控层（VMM）
                https://docs.microsoft.com/zh-cn/windows-hardware/drivers/
                network/overview-of-single-root-i-o-virtualization--sr-iov-
                    为每个 PF 和 VF 分配一个唯一的 PCI Express 请求者 ID (RID) ，
                    该 ID 允许 i/o 内存管理单元 (IOMMU) 区分不同的流量流，
                    并在 PF 与 VFs 之间应用内存和中断转换。
            使用SR-IOV先决条件
                需要 CPU 支持 Intel VT-x 和 VT-d 并在 BIOS 中已启用。
                硬件设备需要支持 SR-IOV 功能。
                需要 QEMU/KVM 的支持
            SR-IOV 部署
                1. 在所有需要配置 SR-IOV 的计算级上开启 Intel VT-d 和 SR-IOV 配置
                    在BIOS中可以打开CPU的VT-x/VT-d功能
                    如果不想重启电脑，我们如何判断是否开启了CPU的虚拟化支持呢？
                    在Linux系统，可以通过下面的方法判断。
                    ● 如何判断CPU是否支持VT-x技术？ 
                    如果grep vmx /proc/cpuinfo执行结果flags中有 vmx ，
                    那么该CPU支持VT-x技术(只说明有此功能，不代表已经打开此功能)
                    如果要使用它，还需要打开CPU的VT-x功能
                    ● 如何判断CPU是否开启了VT-x技术？
                    Intel的VT-x功能是通过IA32_FEATURE_CONTROL寄存器控制的，
                    我们可以使用rdmsr命令读取寄存器 IA32_FEATURE_CONTROL 
                    (address 0x3a)来判断是否开启了VT-x功能
                    若读出值为３和５表示打开了VT-x功能。
                    注：使用rdmsr命令前，先要加载msr驱动（msr-tools）
                    另外，如果 VT-d / IOMMU 被启用，
                    Linux 在启动过程中会配置 DMA重映射，
                    所以简单的方法是在 dmesg 里查找 DMAR 相关项。
                2. 将以下参数添加到 /etc/default/grub 中的 GRUB_CMDLINE_LINUX 配置项中
                    intel_iommu=on
            https://www.itdaan.com/blog/2015/10/13/16396f4d656ce308be778a67ed77bf32.html
            https://www.eet-china.com/mp/a109263.html  
                SR-IOV的出现，支持了单个物理PCIe设备虚拟出多个虚拟PCIe设备，
                然后将虚拟PCIe设备直通到各虚拟机，以实现单个物理PCIe设备支撑多虚拟机的应用场景
                SR-IOV基本结构
                    SR-IOV是在PCIe规范的基础上实现的，
                    SR-IOV协议引入了两种类型功能的概念：
                    物理功能 (Physical Function, PF)和虚拟功能 (Virtual Function, VF)
                基本结构如图：file://imgs/SR-IOV结构.png
                PF
                    PF用于支持 SR-IOV 功能的 PCI 功能，
                    如 SR-IOV 规范中定义，PF 包含 SR-IOV 功能配置结构体，用于管理 SR-IOV 功能。
                    PF 是全功能的 PCIe 功能，可以像其他任何 PCIe 设备一样进行发现、管理和处理。
                    PF 拥有完全配置资源，可以用于配置或控制 PCIe 设备。
                VF
                    VF是与PF关联的一种功能，是一种轻量级 PCIe 功能，
                    可以与PF以及其他VF共享一个或多个物理资源。
                    VF 仅允许拥有用于其自身行为的配置资源。
                VF的BAR空间资源
                    VF不支持端口访问，只支持内存映射
                    VF的BAR空间是PF的BAR空间资源中规划的一部分，参file://imgs/PCIe VF Bar.png
                PF的PCIe扩展配置空间
                    F的PCIe扩展配置空间 SR-IOV Extended Capability支持对SR-IOV功能进行配置
                    如图：file://imgs/PCIe PF的扩展配置空间.png
                    其中SR-IOV Control 字段的bit0位是SR-IOV的使能位，默认为0，表示关闭，
                    如果需要开启SR-IOV功能，需要配置为1。
                    TotalVFs字段表示PCIe Device支持VF的数量。
                    NumVFs字段表示开启VF的数量，此值不应超过PCIe Device支持的VF的数量TotalVFs的值。
                    First VF Offset字段表示第一个各VF相对PF的Routing ID
                    （即Bus number、Device number、Function number）的偏移量。
                    VF Stride字段表示相邻两个VF的Routing ID的偏移量。
                    其他字段含义详见《Single Root I/O Virtualization 
                    and Sharing Specification Revision 1.1》。
                SR-IOV的软件支持
                    Linux系统中PCI驱动框架drivers/pci/iov.c提供了一系列对SR-IOV扩展空间的配置接口函数
                    PCIe Device需要有相应的PF驱动和VF驱动，
                    PF驱动支持配置SR-IOV，VF驱动需要实现相应的PCIe Device的业务功能
中断虚拟化
时间虚拟化    
零碎
    操作系统、驱动和硬件的关系的思考
        https://blog.csdn.net/qq_38880380/article/details/78327825
    printf是如何将字符输出到显示器上的
        https://blog.csdn.net/jump_into_zehe/article/details/106175519
    为什么显卡、声卡甚至鼠标键盘有驱动，而CPU、内存没有驱动？
        https://www.zhihu.com/question/26119207
        真正的处理各种CPU不同架构的程序在HAL里，HAL就是Hardware Abstraction Layer，
        HAL是个dll，不同的cpu，甚至不同的中断处理架构（PIC/APIC），都有不同的HAL，
        支不支持APIC也有不同的HAL。早起Windows安装后带有很多的HAL，还可以切换，
        后期趋近于安装后只有一个HAL。CPU、内存等的不同，都抽象在HAL里，
        但它不是严格意义上的驱动。
    借助在单片机上执行的程序，理解cpu、内存和外设的关系
        cpu和内存和外设的关系就是cpu直接读取执行内存中的指令，并把内存中的数据发送到外设的缓存中，
        外设则以中断的方式将自己的接收或执行情况回馈给cpu。
        那如何把上面的描述与操作系统、驱动结合起来呢？以字符打印为例，
        我们的程序执行一条printf("A")指令，printf时系统API，
        在Linux下，它内部执行的是终端设备的open、write等操作。
        这里的终端设备可以是屏幕，也可以是打印机等。
        操作系统提供的printf内部决定的信息"A"输出到哪种终端（对应不同的驱动），
        而至于write内部是怎么把"A"传到终端设备的缓冲区的，那就是相应的终端驱动所完成的工作了，
        由此可见，虽然在cpu上跑的都是二进制的代码，但这些代码，开始的部分是自己的程序的，
        接着可能就跳转到系统提供的二进制代码部分了，再接着可能就跳转到驱动系统的二进制代码部分了。
    为什么一个cpu会有那么多支持的指令集？
        其实只有一个主指令集，如x86架构的cpu，其主指令集就是x86，其他的指令集都是
        cpu在硬件上进行了更高级的设计，所以进入新的作为补充的指令集（如MMX多媒体指令集）
        来使得cpu发挥更出色的功能。系统不支持某个新增的扩展指令集也能正常功能，
        只是不能受惠于cpu这些扩展指令集带来的cpu性能的增强。
    Windows下查看电脑io设备的内存映射地址/IO映射地址
        https://blog.csdn.net/jiangchao3392/article/details/102475181
        在Windows的设备管理器里，查看显卡的属性页里的“资源”，可以看到内存映射的地址范围
    内存条大小与地址总线位数的关系
        最大内存
            电脑支持的内存位数取决于处理器、系统位数、主板等
            在命令行中输入 wmic memphysical get maxcapacity，
            可得到主板理论支持的最大内存  
            在虚拟机64位系统下测试： 51904512 = 50688M = 49.5G
            在本机上测试： 33554432 = 32G
    系统中的各种表
        中断描述符表 IDT
            IDT表中存放通过各种中断和异常处理函数的入口地址
            当中断或异常发生时，CPU通过他们对应的“向量”索引IDT表，从而进到相应的处理函数
            每个向量占8字节，x86最多有256个向量，故IDT表的最大长度为 8*256 = 2048 字节。
            IDT表的基地址存在IDTR寄存器中。
            在整个系统中，中断描述符表IDT只有一个
            中断描述符表寄存器IDTR指示IDT在内存中的位置
        中断向量表 IVT
            如果在实模式下执行int指令，会自动去访问中断向量表。
            如果在保护模式下执行int指令，则会自动访问中断描述符表。
        全局描述符表 GDT
            GDT的作用是用来提供段式存储机制
            GDT描述系统段，包括操作系统本身
            这种机制是段寄存器和GDT中的描述符共同提供的
            GDT 的线性基地址在 GDTR 中
            GDTR中存了32为的基地址和16为的表界面
            https://zhuanlan.zhihu.com/p/25867829
            段描述符表的结构如图：file://imgs/段描述符表结构.png
        局部描述符表 LDT
            LDT描述每个程序的段，包括代码段、数据段、堆栈段等
            和GDT差不多，区别在于：
            （1）全局（Global）和局部（local）；
            （2）LDT本身是一个段，而GDT不是。
            （3）LDT表存放在LDT类型的段之中，此时GDT必须含有LDT的段描述符
        进程控制项 PCB
            描述进程的当前情况以及控制进程运行的全部信息
            PCB一般包括：
            1.程序ID（PID、进程句柄）：它是唯一的，一个进程都必须对应一个PID。
            2.特征信息：一般分系统进程、用户进程、或者内核进程等
            3.进程状态：运行、就绪、阻塞，表示进程现的运行情况
            4.优先级：表示获得CPU控制权的优先级大小
            5.通信信息：进程之间的通信关系的反映，由于操作系统会提供通信信道
            6.现场保护区：保护阻塞的进程用
            7.资源需求、分配控制信息
            8.进程实体信息，指明程序路径和名称，进程数据在物理内存还是在交换分区（分页）中
            9.其他信息：工作单位，工作区，文件信息等
        内存映射表 页表 Page Table
        内存管理单元 MMU
            它的功能包括虚拟地址到物理地址的转换（即虚拟内存管理）、
            内存保护、中央处理器高速缓存的控制
        页表缓存 TLB
             页表一般都很大，并且存放在内存中，所以处理器引入MMU后，
             读取指令、数据需要访问两次内存：首先通过查询页表得到物理地址，
             然后访问该物理地址读取指令、数据。
             为了减少因为MMU导致的处理器性能下降，引入了TLB，
             TLB是Translation Lookaside Buffer的简称，
             可翻译为“地址转换后援缓冲器”，也可简称为“快表”。
             简单地说，TLB就是页表的Cache，其中存储了当前最可能被访问到的页表项，
             其内容是部分页表项的一个副本
    IVT vs IDT
        IVT 中断向量表    实模式
        IDT 中断描述符表  保护模式 
            他记录了0~255的中断号和调用函数之间的关系
            参：file://imgs/中断描述符结构.png
    IDT vs GDT
        IDT 中断描述符表  保护模式
            他记录了0~255的中断号和调用函数之间的关系
            参：file://imgs/中断描述符结构.png
        GDT 保护模式  用于段寻址
            https://blog.csdn.net/ice__snow/article/details/50654629
            实时模式下，我们对一个内存地址的访问是通过Segment:Offset的方式来进行的
            一个Segment（段）具备两个因素：Base Address和Limit（段的最大长度）
            16-bit系统，Limit不要指定，默认为最大长度64KB（2^16）
            我们在实际编程的时候，使用16-bit段寄存器CS，DS，SS来指定Segment
            CPU将段寄存器中的数值向左偏移4-bit，放到20-bit的地址线上就成为20-bit的Base Address
            在保护模式下仍然使用Segment:Offset“的寻址方式，但其中的含义有所不同
            到了保护模式，内存的管理模式分为两种，段模式和页模式，其中页模式也是基于段模式的。
            也就是说，保护模式的内存管理模式事实上是：纯段模式和段页式
            对于段模式来讲，访问一个内存地址仍然使用Segment:Offset的方式
            在保护模式下，对一个段的描述则包括3方面因素：[Base Address, Limit, Access]
            它们加在一起被放在一个64-bit长的数据结构中，被称为段描述符
            这种情况下，如果我们直接通过一个64-bit段描述符来引用一个段的时候，
            就必须使用一个64-bit长的段寄存器装入这个段描述符
            但Intel为了保持向前兼容，将段寄存器仍然规定为16-bit
            解决的方法就是把这些长度为64-bit的段描述符放入一个数组中，
            而将段寄存器中的值作为下标索引来间接引用，这个全局的数组就是GDT
            事实上，在GDT中存放的不仅仅是段描述符，还有其它描述符
            段描述符表的结构如图：file://imgs/段描述符表结构.png
    GDT vs LDT
        https://blog.csdn.net/billpig/article/details/5833980
        https://www.minazuki.cn/post/blog_os/blog_os-1btagnqm2aabq/blog_os-1btcn43h11jt3/
        寄存器GDTR用来存放GDT的入口地址
        程序员将GDT设定在内存中某个位置之后，可以通过LGDT指令将GDT的入口地址装入此积存器
        从此以后，CPU就根据此寄存器中的内容作为GDT的入口来访问GDT了
        由GDTR访问全局描述符表是通过“段选择子”（实模式下的段寄存器）来完成的
        段选择子包括三部分：描述符索引（index）、TI、请求特权级（RPL）
        实模式下的段寄存器(如cs,ss等)在保护模式下叫段选择子(Selector)
        cpu访问代码段时，使用的是CS，访问数据段时，使用的是DS，访问堆栈段使用的是SS
        无论使用CS，还是DS、SS，格式都是一样的，之所以区分，猜测应该是为了更方便的保护上下文
    保护模式 vs 实模式
        cr0寄存器控制进入保护模式
        保护模式出现的原因是：保护进程地址空间。
        这样，就产生了一个结果：两种模式下程序的寻址方式发生了变化。
        实模式
            CPU复位（reset）或加电（power on）的时候就是以实模式启动，在这个时候处理器以实模式工作，
            不能实现权限分级，也不能访问20位以上的地址线，也就是只能访问1M内存。
            之后一般就加载操作系统模块，进入保护模式。
        保护模式
            尽管 8086 是 16 位的处理器，但它也是 32位架构内的一部分。
            原因在于， 32 位的处理器架构是从 8086 那里发展来的，是基于 8086 的，具有延续性和兼容性。
            32位处理器有自己的 32 位工作模式。在本系列文章中，保护模式其实就是32位模式。
            在这种模式下，可以完全、充分地发挥处理器的性能。
            同时，在这种模式下，处理器可以使用它全部的 32根地址线，能够访问4GB 内存。
            其实在保护模式下地址的表示方式与实模式是一样的，都是：段（segment）:偏移（offset），
            不过保护模式下，“段”的概念发生了根本性的改变。
            实模式下，段值还是可以看作是地址的一部分
            而保护模式下，虽然段值仍然由原来的cs、ds等寄存器表示，但此时它仅仅变成了一个索引，
            这个索引指向了一个GDT数据结构的一个表项，表项中详细定义了段的起始地址、界限、属性等内容
    用户态和内核态 vs 实模式和保护模式 vs 特权指令和非特权指令
        用户态和内核态 vs 特权指令和非特权指令
            https://www.cnblogs.com/hongzg1982/articles/2132568.html
            内核态与用户态是操作系统的两种运行级别
            intel cpu提供Ring0-Ring3三种级别的运行模式
            Windows只使用其中的两个级别RING0和RING3，RING0只给操作系统用，RING3谁都能用。
            如果普通应用程序企图执行RING0指令，则Windows会显示“非法指令”错误信息。
            x86处理器是通过Ring级别来进行访问控制的
            实现这种限制的方法是在CPU中设置了一个代表特权级别的状态字
            （对x86的CPU来说就是cs寄存器中的DPL字段）
            大多数指令可以同时使用于R0层和R3层，但有些和系统设置相关的指令却只能在R0层被使用，
            或者在R3层的使用受到限制，主要有下面这些：
                lgdt：加载GDT寄存器
                lldt：加载LDT寄存器
                ltr：加载任务寄存器
                lidt：加载IDT寄存器
                mov：加载和存储控制寄存器、调试寄存器时受限
                lmsw：加载机器状态字
                clts：清除cr0中的任务切换标记
                invd：缓冲无效，并不写回
                wbinvd：缓冲无效，并写回
                invlpg：无效TLB入口
                hlt：停止处理器
                rdmsr：读模式指定寄存器
                wrmsr：写模式指定寄存器
                rdpmc：读取性能监控计数器
                rdtsc：读取时间戳计数器
             最后2条指令rdpmc和rdtsc，在cr4的位4（PCE）和位2（TSD）被设置的情况下可以同时被R0层和R3层调用。
             任何违反上面规定的操作，在Windows下都可能会产生通用保护故障的异常。
             另外，还有些所谓的IO敏感指令，包括：
                cli：关闭中断
                sti：开启中断
                in：从硬件端口读
                out：往硬件端口写
            这些指令在R0层可以直接被使用，在R3层被使用的时，候还要检查IO许可位图，综合判断是否允许调用。
            Linux进程的4GB地址空间，3G-4G部分大家是共享的，是内核态的地址空间
            这里存放在整个内核的代码和所有的内核模块，以及内核所维护的数据
            Ring3状态不能访问Ring0的地址空间，包括代码和数据
            用户运行一个程序，该程序所创建的进程开始是运行在用户态的，
            如果要执行文件操作，网络数据发送等操作，必须通过 write，send等系统调用，
            这些系统调用会调用内核中的代码来完成操作，这时，必须切换到Ring0，
            然后进入3GB-4GB中的内核地址空间去执行这些代码完成操作，
            完成后，切换回Ring3，回到用户态。
            这样，用户态的程序就不能随意操作内核地址空间，具有一定的安全保护作用。
            当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态
            当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。
            此时处理器处于特权级最高的（0级）内核代码中执行
        CPL RPL与DPL
            https://blog.csdn.net/qq_37414405/article/details/84535145
            RPL（Request Privilege level）进程对段访问的请求权限
                其存在于段选择子的bit 0 和 bit 1两位中（可称“段描述符表”的各项为“段选择子”）
        实模式和保护模式
            实模式和保护模式是内存的两种模式。
            linux中保护模式下可以访问4G的内存空间，实模式下可以访问的要少的多。
            实模式是指指令使用的都是直接的内存地址，而保护模式下的地址需要转换才是实际的内存地址，
            这个方法的最大好处是可以控制对一些敏感数据的访问。        
            DOS就是实模式的，现在的Winows、Unix之类东西都是保护模式的
            至于保护模式，特性挺多，如线性地址、虚拟内存、权限保护之类的东西都是CPU提供的功能
            为开发现代操作系统提供了很多便利。
            x86CPU在初始化的时候都是先进入实模式，然后操作系统再切换到保护模式
    VT-d vs VT-c
        https://www.cnblogs.com/zafu/p/15557488.html
        VT-d 是可以将一个物理网卡直通给一个虚拟机
        现在的 VT-c 就很厉害了，可以将一个物理网卡分成十份，
        分别直通给10个虚拟机，并且这十份都是隔离互不影响的
    KVM
    KVM 和 QEMU
    VFIO
        https://blog.csdn.net/hx_op/article/details/104029622
            VFIO就是内核针对IOMMU提供的软件框架，支持DMA Remapping和Interrupt Remapping
            VFIO利用IOMMU这个特性，可以'屏蔽物理地址对上层的可见性'，
            可以用来开发用户态驱动，也可以实现设备透传
            Group
                group 是IOMMU能够进行DMA隔离的最小硬件单元，
                一个group内可能只有一个device，也可能有多个device，
                这取决于物理平台上硬件的IOMMU拓扑结构。 
                设备直通的时候一个group里面的设备必须都直通给一个虚拟机。 
                不能够让一个group里的多个device分别从属于2个不同的VM，
                也不允许部分device在host上而另一部分被分配到guest里， 
                因为就这样一个guest中的device可以利用DMA攻击获取另外一个guest里的数据，
                就无法做到物理上的DMA隔离。
            Container
                对于虚拟机，Container 这里可以简单理解为一个VM Domain的物理内存空间。
                对于用户态驱动，Container可以是多个Group的集合。
        https://www.jianshu.com/p/bfbb5095d8fd
            许多硬件平台提供DMA和中断重映射功能，以确保I/O设备只在分配给它们的域内访问内存，
            一般这样的硬件单元称为IOMMU
            VFIO驱动程序是一个与IOMMU具体硬件无关的框架，
            可以支持x86架构的VT-D、AMD-Vi，POWER PE，ARM等各种IOMMU硬件架构
            使用VFIO可以实现安全的，非特权的，用户空间驱动程序
            相较于VFIO，用UIO框架实现的用户空间驱动，将不受IOMMU的保护，
            且中断支持有限，并且需要root权限运行
            VFIO层有Group，Device及IOMMU的概念
                Device
                    Device是所有IO驱动的主体。
                IOMMU
                    IOMMU硬件可以将设备的可访问物理内存空间进行彼此的隔离，
                    当在IOMMU硬件中创建好IO虚拟地址到物理地址的映射后，
                    设备可使用IO虚拟地址访问物理内存。
                group
                    有时候，相关的一组设备可能会使用同一块内存空间。
                    因此IOMMU给内存创建隔离区的最小粒度不是Device，而是group。
                    因此，group也是VFIO使用的最小粒度。
                    在使用页表的IOMMU中，可以在不同group之间共享一组页表，
                    因此，VFIO使用container 类，该类可以包含一个或多个group。
                    只需打开/dev/vfio/vfio字符设备即可创建container
                    容器本身提供的功能很少，除了对版本和扩展查询接口外，其他所有接口都被锁定
                    用户需要在容器中添加一个group以获得下一级功能，即IOMMU
                    不同架构的IOMMU可能提供的接口不同，因此container并未提供统一接口访问IOMMU
                    为此，用户首先需要标识（即后文的/dev/vfio/{group}）与所需设备关联的group
                    通过绑定设备到VFIO驱动程序，
                    为该组添加一个新的VFIO组/dev/VFIO/{group}字符设备文件接口，
                    其中{group}是设备所属的IOMMU组号
                    一旦组准备好，就可以通过打开VFIO组字符设备（/dev/VFIO/$group）
                    并使用 VFIO_GROUP_SET_CONTAINER ioctl，
                    传递先前打开的容器文件的文件描述符，将其添加到容器中。
                    如果需要，并且IOMMU驱动程序支持在组之间共享IOMMU上下文，
                    则可以将多个组设置为同一容器。
                    将一个组（或多个组）附加到容器后，剩余的ioctl将变为可用，
                    从而可以访问VFIO IOMMU接口，
                    还可以使用VFIO组文件描述符上的ioctl为组中的每个设备获取文件描述符。
                    VFIO操作device的API包括用于设备的描述（PCI配置空间）、
                    IO区域（BAR空间）及其在设备描述符上的读/写/mmap偏移量的ioctl，
                    以及用于描述和注册中断通知的机制
        https://cloud.tencent.com/developer/article/1816469
            ————VFIO研究
            VFIO的全称是Virtual Function IO
            由于VFIO是将设备直接透传给虚拟机,
            所以Guest中与该设备相关的IO性能会大幅提高,接近native性能.
            研究目的
                研究利用-device vfio-pci的方式将PCI透传到虚拟机中后,
                在虚拟机中访问PCI设备的配置空间,MMIO寄存器,IO Port的流程是怎样的.
            VFIO原理
                VFIO把设备通过IOMMU映射的DMA物理内存地址映射到用户态中,
                让用户态程序可以自行操纵数据的传输,还可以自行注册中断处理函数,
                从而在用户态下实现设备的驱动程序.
                因此VFIO的基础是IOMMU.
            VFIO Container
                在IOMMU_GROUP的基础上,VFIO封装了一层Container Class,
                Container的作用是,当我们想在不同的IOMMU_GROUP之间共享TLB和
                page tables(用于地址翻译的页表)时,
                就将这些group放到同一个container中,
                因此Container可以看做是IOMMU_GROUP的集合.
            虚拟化中VFIO的应用
                这里演示一个将网卡设备利用VFIO透传到虚拟机中的例子
                需要注意的是,利用VFIO将PCI设备透传到虚拟机之后,Host将无法使用该设备.
                利用默认qemu选项启动虚拟机后,查看lspci的输出,
                可以看到虚拟机默认的网卡是Intel的82540EM,这是由Qemu-kvm软件模拟出来的一款网卡
                下面将Host的PCI网卡透传到虚拟机
                首先在Host上使用lspci查看拥有的PCI网卡型号,
                可以看到Realtek的一款网卡,该网卡的PCI标记为06:00.0.
                #要使用VFIO,必须在Linux启动时添加启动项intel_iommu=on,因为VFIO的底层依赖IOMMU.
                加载VFIO-PCI modul : sudo modprobe vfio-pci
                    modprobe可载入指定的个别模块，或是载入一组相依的模块。
                    modprobe会根据depmod所产生的相依关系，决定要载入哪些模块。
                    若在载入过程中发生错误，在modprobe会卸载整组的模块。
                    vfio_pci是VFIO对pci设备驱动的统一封装，
                    它和用户态进程一起配合完成设备访问直接访问，
                    具体包括PCI配置空间模拟、PCI Bar空间重定向，Interrupt Remapping等
                    如果加载成功,可以在dmesg中看到相关log：
                    dmesg | grep VFIO
                    [ 2916.038696] VFIO - User Level meta-driver version : 0.3
                查看网卡所在的 IOMMU Group
                    $ readlink /sys/bus/pci/devices/0000:06:00.0/iommu_group
                    ../../../../kernel/iommu_groups/3
                    可以看到该Realtek网卡位于iommu_group的group3
                查看设备所在iommu_group的所有设备
                    $ ls /sys/bus/pci/devices/0000:06:00.0/iommu_group/devices/
                    0000:00:05.0  0000:00:05.1  0000:06:00.0
                    可以看到在iommu_group3中,除了该Realtek网卡,还有2个设备.
                将设备与对应的驱动解绑
                    为了将设备透传到虚拟机中,需要将设备与其对应的驱动解绑,
                    这样该设备就可以使用VFIO的驱动了
                    注意,不仅要将要透传的设备解绑,还要将与设备同iommu_group的设备都解绑,才能透传成功.
                    $ echo 0000:06:00.0 | sudo tee /sys/bus/pci/devices/0000:06:00.0/driver/unbind
                    0000:06:00.0
                    $ echo 0000:00:05.0 | sudo tee /sys/bus/pci/devices/0000:00:05.0/driver/unbind
                    0000:00:05.0 
                    $ echo 0000:00:05.1 | sudo tee /sys/bus/pci/devices/0000:00:05.1/driver/unbind
                    0000:00:05.1
                查看设备的Vendor和DeviceID
                    $ lspci -n -s 06:00.0
                    06:00.0 0200: 10ec:8168 (rev 15)
                    可以看到该Realtek网卡的Vendor为10ec,DeviceID为8168.
                将设备绑定到vfio-pci module
                    $  echo 10ec 8168 | 
                    sudo tee /sys/bus/pci/drivers/vfio-pci/new_id
                    10ec 8168
                    可以通过ls /dev/vfio查看是否绑定成功,
                    如果绑定成功,/dev/vfio目录下会出现该device所属的iommu_group号.
                    ls /dev/vfio
                    3 vfio
                    这里的3就是我们要透传的Realtek网卡所在的iommu_group号.
                启动虚拟机
                    $ sudo x86_64-softmmu/qemu-system-x86_64 
                    -m 4096 -smp 4 -hda ~/ewan/Workspace/img/Ubuntu18.04_loop.img
                    -enable-kvm -cpu host -device vfio-pci,host=06:00.0
                    注：这里使用的是从源码编译的qemu,但是从apt-get安装的的qemu也是可以的.
                进入guest之后运行lspci,可以看到相比之前的PCI设备,多了一个Realtek网卡,
                这个网卡和host上的信息一模一样,证明将网卡透传到Guest成功
            https://cloud.tencent.com/developer/article/1942108
                ————深入理解VFIO驱动框架
                VFIO（Virtual Function I/O）驱动框架是一个用户态驱动框架
                在intel平台它充分利用了VT-d等技术提供的
                DMA Remapping和Interrupt Remapping特性
                VFIO是一个可以安全的把设备I/O、中断、DMA等暴露到用户空间，
                用户态进程可以直接使用VFIO驱动访问硬件，
                从而可以在用户空间完成设备驱动的框架
    IOMMU
        IOMMU主要功能包括DMA重映射和中断重映射，
        IOMMU可以将一个设备访问地址转换为存储器地址，从而实现设备的透传（客户机可以直接访问设备）
        https://cloud.tencent.com/developer/article/1816469
        如果没有IOMMU,DMA也能直接访问RAM中的内容,
        但是让DMA没有限制地访问RAM是一件很危险的事情,
        而IOMMU能够对这个过程加以限制,当DMA访问的地址合法时,IOMMU才返回正确的数据
        除了翻译地址的功能,IOMMU还能对硬件中断进行重映射,
        达到屏蔽部分中断,或自定义中断处理函数的目的.
        基于地址翻译和硬件中断重映射两大功能,IOMMU就具有了隔离设备的能力,
        这提高了设备访问RAM时和设备发出中断时的安全性.
        除了将单个设备隔离的功能外,IOMMU还能隔离一组设备,如隔离PCI桥上的几个设备,
        所以IOMMU还有一个概念,叫做IOMMU_GROUP, 代表一组被隔离的设备的集合.
        通过把host的device和对应driver解绑,然后绑定在VFIO的driver上,
        就会在/dev/vfio/目录下出现一个group,这个group就是IOMMU_GROUP号,
        如果需要在该group上使用VFIO,需要将该group下的所有device与其对应的驱动解绑.
    linux开启IOMMU方法
        https://blog.csdn.net/hbuxiaofei/article/details/106566348
        1. 修改/etc/default/grub, 调整GRUB_CMDLINE_LINUX内容
           GRUB_CMDLINE_LINUX="crashkernel=auto rhgb quiet amd_iommu=on iommu=pt"
           (amd_iommu/intel_iommu/AuthenticAMD)
           内核引导参数IOMMU与INTEL_IOMMU有何不同？
               http://linuxperf.com/?p=84
               Linux kernel有两个引导参数(boot parameter)与iommu有关：
               iommu=[on/off] 和 intel_iommu=[on/off]
               参数iommu控制的是GART(Graphics Address Remapping Table) iommu功能，
               参数intel_iommu控制的是基于Intel VT-d的iommu功能
               GART最初是为了方便图形芯片直接读取内存而设计的：
               使用地址转译功能将收集到内存中的数据映射到一个图形芯片可以“看”到的地址。
               这个地址转译功能自然也可以充当IOMMU，
               于是GART被Linux kernel用来帮助传统的32位PCI设备访问可寻址范围之外的内存区域。
               GART iommu有局限性（比如仅限于显存范围内），不具备Intel IOMMU的完整功能。
               “iommu”参数默认是打开的
               注：GART iommu功能是按需激活的，并有前提条件，
               比如系统内存必须在3GB以上、而且只对有限的设备
        2. 重新创建引导
           如果服务器时UEFI启动    grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg
           普通模式启动            grub2-mkconfig -o /boot/grub2/grub.cfg
        3. 在BIOS中开启IOMMU选项，如果网卡有对应的虚拟化选项需同步开启
        4. 查看状态                dmesg | grep -E "DMAR|IOMMU"
    南桥 vs 北桥
        北桥
            主板上离CPU最近的一块芯片，负责与CPU的联系并控制内存，
            在处理器与PCI总线、DRAM、AGP和L2高速缓存之间建立通信接口起到很大的作用
            北桥芯片就是主板上离CPU最近的芯片，
            这主要是考虑到北桥芯片与处理器之间的通信最密切，
            为了提高通信性能而缩短传输距离。
            北桥芯片负责与CPU的联系并控制内存AGP数据在北桥内部传输，
            提供对CPU的类型和主频、系统的前端总线频率、内存的类型和最大容量、
            AGP插槽、ECC纠错等支持，
            整合型芯片组的北桥芯片还集成了显示核心。
            北桥起到的作用非常明显，在电脑中起着主导的作用，
            所以人们习惯的称为主桥（Host Bridge）
        南桥
            南桥芯片的发展方向主要是集成更多的功能，
            例如网卡、RAID、IEEE 1394、甚至WI-FI无线网络等等。
            南桥芯片（South Bridge）是主板芯片组中除了北桥芯片以外最重要的组成部分，
            一般位于主板上离CPU插槽较远的下方，
            PCI插槽的附近，这种布局是考虑到它所连接的I/O总线较多，
            离处理器远一点有利于布线，而且更加容易实现信号线等长的布线原则。
            相对于北桥芯片来说，南桥芯片数据处理量并不算大，
            所以南桥芯片一般都不必采取主动散热，有时甚至连散热片都不需要。
            南桥芯片主要是负责I/O接口等一些外设接口的控制、IDE设备的控制及附加功能等等。
            CPU也没有必要实时的去和所有的外围设备通信，
            这时候呢就需要有一个可能代替CPU与设备通信的角色，这个角色就是南桥芯片组。
        两者关系
            不同的南桥芯片可以搭配不同的北桥芯片，虽然其中存在一定的对应关系，
            但是只要连接总线相符并且针脚兼容，主板厂商完全可以随意选择
            北桥用于CPU和内存、显卡、PCI交换数据而南桥主要是负责IO。
        消失的北桥
            https://zhuanlan.zhihu.com/p/375804757
            现在的CPU制造工艺越来越先进，集成度越来越高，内存控制器已被集成到CPU里，
            就连显卡也被收进CPU了（就是我们所说的核显），而PCIE控制器收归南桥管理了，
            因此北桥芯片组的功能被瓜分了，所以现在的Intel芯片组把北桥取消掉只剩南桥了，
            而AMD也只有早期的主板还保留着北桥和南桥
            #北桥从十多年前就被合并到CPU中了
            PCIE是直接连接在CPU的PCIE控制器上，所以才能保证它的延迟非常非常的低、速度非常非常的快。
            只是PCIE控制器能提供的PCIE通道的数量是有限度的，CPU同样也要考虑电路的设计成本，
            而我们电脑上有这么多的设备都要去走PCIE通道去连接CPU，
            这对主板的布线来说简直是一个地狱级的挑战！
            但是CPU也没有必要实时的去和所有的外围设备通信，
            这时候呢就需要有一个可能代替CPU与设备通信的角色，这个角色就是南桥芯片组。
            在电脑里所有设备中内存对于速度要求是非常高的，
            所以内存是直接跟CPU对接的，同样对速度要求高的还有PCIE也是直接跟CPU对接。
            其它的设备比如声卡、网卡、固态硬盘、机械硬盘、USB等等这种对实时通讯要求不是很高的设备
            就全部接入南桥芯片组，南桥收集好了数据后再传输给CPU处理。这就是南桥芯片组的工作原理。
            CPU上的PCIE3.0控制器是X16的，而目前的显卡基本都是X16的，所以一条显卡就把直连CPU的PCIE
            的16条通道都沾满了，那如何再插一个M.2 PCIE X4的固态硬盘呢？
            其实M.2用的PCIE通道不是直连CPU的，M.2用的是来自南桥芯片组的24条非直连的PCIE3.0通道。
            在CPU与南桥之间是通过DMI3.0总线来连接的，这个总线其实就是PCIE3.0X4
            也就是说CPU和南桥之间的数据带宽上限就是PCIE3.0X4（4Gb/S）
            这也就解释了为什么市场上的固态硬盘包括M.2接口的基本用的是PCIE3.0X4的总线了，
            因为M.2是用的南桥的PCIE通道所以它的速度是不可能超过CPU和南桥之间的带宽的。
            也就是说所有连接到南桥的设备其速度都无法超过4GB/S，
            所以当前最快的民用M.2固态硬盘970PRO顺序读写也只能卡在3.6GB/S左右，无法突破4GB/S
    RAM / SRAM /DRAM
        RAM分SRAM与DRAM，前者是静态RAM，不需要刷新，速度快，容量小，造价高，
        老式计算机使用。后者是动态RAM，要进行刷新，速度慢，容量大，
        造价低，现代计算机使用。
        SRAM速度非常快，是目前读写最快的存储设备了，但是它也非常昂贵，
        所以只在要求很苛刻的地方使用，譬如CPU的一级缓冲，二级缓冲
        DRAM保留数据的时间很短，速度也比SRAM慢，不过它还是比任何的ROM都要快，
        但从价格上来说DRAM相比SRAM要便宜很多，计算机内存就是DRAM的
        DRAM分为很多种，常见的主要有FPRAM/FastPage、EDORAM、SDRAM、
        DDR RAM、RDRAM、SGRAM以及WRAM等
        DDR RAM（Double-Date-Rate RAM）也称作DDR SDRAM，
        这种改进型的RAM和SDRAM是基本一样的，
        不同之处在于它可以在一个时钟读写两次数据，
        这样就使得数据传输速度加倍了。
        这是目前电脑中用得最多的内存，而且它有着成本优势
    Flash/NOR Flash/NAND Flash
        目前Flash主要有两种NOR Flash和NADN Flash。
        NOR Flash的读取和我们常见的SDRAM的读取是一样，
        用户可以直接运行装载在NOR FLASH里面的代码，
        这样可以减少SRAM的容量从而节约了成本。N
        AND Flash没有采取内存的随机读取技术，
        它的读取是以一次读取一块的形式来进行的，
        通常是一次读取512个字节，采用这种技术的Flash比较廉价。
        用户不能直接运行NAND Flash上的代码，
        因此好多使用NAND Flash的开发板除了使用NAND Flah以外，
        还作上了一块小的NOR Flash来运行启动代码。 
        一般小容量的用NOR Flash，因为其读取速度快，
        多用来存储操作系统等重要信息，而大容量的用NAND FLASH，
        最常见的NAND FLASH应用是嵌入式系统采用的DOC（Disk On Chip）
        和我们通常用的"闪盘"，可以在线擦除。
    CPU频率、外频、前端总线频率、内存频率
        https://blog.csdn.net/wfq_1985/article/details/6700838
            CPU的外频是CPU乃至整个计算机系统的基准频率，单位是MHz（兆赫兹）
            算机系统中大多数的频率都是在外频的基础上，乘以一定的倍数来实现。
            前端总线，通常是CPU的外频的2到4倍，
            也没有固定的倍数，和cpu型号及主板芯片组有关
            系统总线，这个应该是个比较笼统的概念，可以代指系统中所有总线
            （前端总线，agp总线，pci总线等），也有时候指南北桥之间的总线，
            也没有固定的频率和算法
            CPU频率 ＝ 外频×倍频  （or 主频 ＝ 外频×倍频）
            IntelCPU前端总线=外频*4（MHz）
            AMDCPU前端总线=外频*2（MHz）
            总线通俗的说，就是多个部件间的公共连线,用于在各个部件之间传输信息
            总线的种类很多，前端总线的英文名字是Front Side Bus，通常用FSB表示，
            是将CPU连接到北桥芯片的总线。
            CPU是通过前端总线（FSB）连接到北桥芯片，
            进而通过北桥芯片和内存、显卡交换数据
            至于内存来说，一般会有三种频率来对其描述:核心频率/时钟频率/数据传输速率
            数据传输速率就是标在内存条上的频率，如DDR333 和DDR400
            平时说的内存频率默认就是指数据传输速率
            内存的核心频率是本身所固有的频率，而时钟频率就是我们所说的外频
        https://zhidao.baidu.com/question/39826402.html
            总线频率是主板的，不是CPU的
            总线是将计算机微处理器与内存芯片以及与之通信的设备连接起来的硬件通道
            前端总线将CPU连接到主内存和通向磁盘驱动器、
            调制解调器以及网卡这类系统部件的外设总线
            前端总线（FSB）频率是直接影响CPU与内存直接数据交换速度
        前端总线
            https://baike.baidu.com/item/fsb/427104
            https://baike.baidu.com/item/%E5%A4%96%E9%A2%91/108610
                外频与前端总线（FSB）频率很容易被混为一谈。
                前端总线的速度指的是CPU和北桥芯片间总线的速度，
                更实质性的表示了CPU和外界数据传输的速度
                外频与前端总线（FSB）频率很容易被混为一谈。
                前端总线的速度指的是CPU和北桥芯片间总线的速度，
                更实质性的表示了CPU和外界数据传输的速度。
                而外频的概念是建立在数字脉冲信号震荡速度基础之上的，
                也就是说，100MHz外频特指数字脉冲信号在每秒钟震荡一万万次，
                它更多的影响了PCI及其他总线的频率。
        外频
            https://baike.baidu.com/item/%E5%A4%96%E9%A2%91/108610
            CPU的外频，通常为系统总线的工作频率（系统时钟频率）
            CPU与周边设备传输数据的频率，具体是指CPU到芯片组之间的总线速度
            在计算机主板上，以CPU为主，内存和各种外围设备为辅，
            有许多设备要共同在一起工作,些设备之间的联络，数据的交换，
            都必须正确无误，分秒不差。因此，它们必须要有一个固定的
            时钟来做时间上的校正，协调或者参考.
            这个时钟由主板上的时钟发生器产生，就是所谓的外频
            计算机系统中大多数的频率都是在外频的基础上，乘以一定的倍数来实现
            说到处理器外频，就要提到与之密切相关的两个概念：倍频与主频，
            主频就是CPU的时钟频率；倍频即主频与外频之比的倍数。
            主频、外频、倍频，其关系式：主频=外频×倍频。
            一个CPU默认的外频只有一个，主板必须能支持这个外频
    AGP总线
        AGP，全称为加速图像处理端口（Accelerated Graphics Port）
        供显卡使用，主要应用在三维电脑图形的加速上
        AGP是在1997年由Intel提出，是从PCI标准上建立起来，是一种显卡专用接口
        严格的说，AGP不能称为总线，它与PCI总线不同，
        因为它是点对点连接，即连接控制芯片和AGP显示卡，
        但在习惯上我们依然称其为AGP总线
        AGP接口是基于PCI 2.1 版规范并进行扩充修改而成，工作频率为66MHz。
        AGP接口的发展经历了AGP1.0(AGP1X、AGP2X)、AGP2.0(AGP Pro、AGP4X)、
        AGP3.0(AGP8X)（2000年8月）等阶段，
        其传输速度也从最早的AGP1X的266MB/S的带宽发展到了AGP8X的2.1GB/S。    
        现在已经被淘汰了(早就换代了)，
        目前的显卡插槽为pci-e的，无论是速度还是带宽都比agp的快
    SoC
        https://www.jianshu.com/p/ab99d835b55a
        SoC的全称叫做：System-on-a-Chip，中文的的意思就是“把系统都做在一个芯片上”
        如果在PC时代我们说一个电脑的核心是CPU，
        那么在智能终端时代，手机的核心就是这个SoC
        这么说是因为SoC上集成了很多手机上最关键的部件，
        比如CPU、GPU、内存、也就说虽然它在主板上的存在是一个芯片，
        但是它里边可是由很多部件封装组成的。
        与单片机相比，单片机的内部结构是固定的，标准的，
        而soc则是泛指一类产品、一种技术，
        它将更多的功能模块(也叫外设)集成到一块集成电路中去，
        使其更加丰富封装成一颗独立的芯片。如某颗SOC内部集成了I/O(也叫GPIO)控制器，
        timer，还集成了NAND flash控制器，SD卡控制器等外设。
    FPGA
        https://baike.baidu.com/item/FPGA/935826
        FPGA（Field Programmable Gate Array）现场可编程逻辑门阵列
        是在PAL （可编程阵列逻辑）、GAL（通用阵列逻辑）等
        可编程器件的基础上进一步发展的产物
        
        
        
        